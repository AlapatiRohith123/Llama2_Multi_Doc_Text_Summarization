{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [],
   "source": [
    "# This Python 3 environment comes with many helpful analytics libraries installed\n",
    "# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n",
    "# For example, here's several helpful packages to load\n",
    "\n",
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the read-only \"../input/\" directory\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n",
    "\n",
    "import os\n",
    "for dirname, _, filenames in os.walk('/kaggle/input'):\n",
    "    for filename in filenames:\n",
    "        print(os.path.join(dirname, filename))\n",
    "\n",
    "# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n",
    "# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers einops accelerate langchain bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "with open('/kaggle/input/datasetSecure/data.json', 'r') as file:\n",
    "    datasec = json.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "token=datasec[\"token\"]\n",
    "command = f\"huggingface-cli login --token {token}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!{command}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install sentencepiece"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install torch -U\n",
    "!pip install transformers accelerate git+https://github.com/kashif/diffusers.git@a3dc21385b7386beb3dab3a9845962ede6765887"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import HuggingFacePipeline\n",
    "from transformers import AutoTokenizer\n",
    "import transformers\n",
    "import torch\n",
    "from transformers import pipeline\n",
    "torch.backends.cuda.enable_mem_efficient_sdp(False)\n",
    "torch.backends.cuda.enable_flash_sdp(False)\n",
    "model1 = \"\"\n",
    "# model = \"soundarya2873/llama-2-7b-chat-finetuned1\"\n",
    "model=\"meta-llama/Llama-2-7b-chat-hf\"\n",
    "# model=\"alapatirohith/llama-2-7b-chat-finetuned1_\"\n",
    "# model=\"Dhanunjaysuppa/llama-2-7b-chat-finetuned1_fun\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model)\n",
    "\n",
    "pipeline = pipeline(\"text-generation\",\n",
    "                model=model,\n",
    "                tokenizer= tokenizer,\n",
    "                torch_dtype=torch.bfloat16,\n",
    "                device_map=\"auto\",\n",
    "                max_new_tokens = 512,\n",
    "                do_sample=True,\n",
    "                top_k=30,\n",
    "                num_return_sequences=1,\n",
    "                eos_token_id=tokenizer.eos_token_id\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = HuggingFacePipeline(pipeline = pipeline, model_kwargs = {'temperature':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id = \"65d9e14c9b857084a037f70e\"\n",
    "text = r\"\"\"11  \n",
    "  \n",
    " \n",
    "UNIT  – 1 \n",
    "     \n",
    " \n",
    " PHYSICAL DESIGN OF IoT  \n",
    "    \n",
    "ii) IoT Protocol  \n",
    " \n",
    "                       \n",
    "   \n",
    "      \n",
    "Link Layer  \n",
    "The Link Layer protocol manages the physical transmission of data over the network's medium and \n",
    "facilitates communication between hosts within the loc al network connection by encoding and \n",
    "signaling packets through hardware devices.  \n",
    " \n",
    "802.3 Ethernet:  \n",
    "802.3 Ethernet encompasses various wired standards for the link layer, like 10BASE5 Ethernet using \n",
    "coaxial cable and 802.3.i for 10 BASET Ethernet over twist ed copper pairs, offering data rates from \n",
    "10 Mb/s to 40 gigabits per second or more. The shared medium, which can be coaxial cable, twisted \n",
    "pair wire, or optical fiber, facilitates communication among all network devices  \n",
    " \n",
    "802.1 - WI-FI:  \n",
    "IEEE 802.11 is a se t of wireless Local Area Network (WLAN) standards that define the link layer. \n",
    "Variants like 802.11a, 802.11b, 802.11g, and 802.11ac operate in different frequency bands —11  \n",
    " 802.11a in the 5 GHz band, and 802.11b, 802.11g, and 802.11ac in the 2.4 GHz and 5 GHz bands.  \n",
    " \n",
    "802.16 WIMAX:  \n",
    "IEEE 802.16, also known as WiMAX, comprises wireless broadband standards with detailed link \n",
    "layer descriptions. WiMAX offers data rates ranging from 1.5 Mb/s to 1 Gb/s, with recent updates \n",
    "delivering speeds of hundreds of megabits per  second for mobile stations  \n",
    " \n",
    "802.15.4 LR -WPAN:  \n",
    "IEEE 802.15.4 sets standards for Low Rate Wireless Personal Area Networks (LR -WPAN), forming \n",
    "the basis for protocols like Zigbee, with data rates beginning at 40 kb/s, ideal for affordable, low -\n",
    "speed communica tion suited to power -constrained devices.  \n",
    " \n",
    "2G / 3G / 4G mobile communications:  \n",
    "These are the different generations of mobile communication standards including second  \n",
    "generation (2G including GSM and CDMA). 3rd Generation (3G including UMTS and  \n",
    "CDMA2000) and 4th generation 4G including LTE.  \n",
    " \n",
    "Network / internet layer : \n",
    " \n",
    "The network layer is responsible for sending IP data  grams from the source network to the destination \n",
    "network, handling host addressing and packet routing using hierarchical IP addressing sc hemes like \n",
    "IPv4 or IPv6.  \n",
    "IPv4, the most widely deployed internet protocol, identifies devices on a network using hierarchical \n",
    "addressing schemes, utilizing a 32 -bit address format capable of accommodating up to 2^32 \n",
    "addresses. Due to the increasing number of connected devices, IPv4 has been succeeded by IPv6.  \n",
    "IPv6: It is the newest versions of internet protocol and successor to IPv4. IPv6 uses 128-bit address \n",
    "schemes that are lost total of 2 128 are 3.4* 10 38 address.  \n",
    "6LoWPAN brings IPv6 protocol to low -power devices with limited processing capabilities, operating \n",
    "in the 2.4 GHz frequency range and providing data transfer rates of up to 50 kb/s.  \n",
    " \n",
    "Transport Layer:  \n",
    "The Transport layer protocols offer end -to-end message transfer capability regardless of the \n",
    "underlying network, establishing connections with or without handshake acknowledgement. They \n",
    "include functions like error control, segmentation, flow control, and congestion control.  \n",
    " \n",
    "UDP, in contrast to TCP, functions as a connection -less protocol, ideal fo r time -sensitive applications \n",
    "needing swift data exchange without connection setup. It operates in a transaction -oriented, stateless \n",
    "manner, without ensuring guaranteed delivery, message ordering, or duplicate elimination.  \n",
    " \n",
    "Application layer :  11  \n",
    " The applicat ion layer protocol facilitates the transmission of data between applications by encoding \n",
    "files and encapsulating them within the transport layer protocol, establishing process -to-process \n",
    "connections via ports.  \n",
    " \n",
    "HTTP  (Hypertext  transfer protocol):  is an app lication layer protocol used for the World Wide \n",
    "Web, allowing clients to send requests to servers using commands like GET, PUT, POST, DELETE, \n",
    "HEAD, TRACE, and OPTIONS, operating on a stateless request -response model, and clients can \n",
    "include web browsers, I oT devices, mobile applications, or other software.  \n",
    " \n",
    "CoAP  (Constrained Application Protocol ): is designed for machine -to-machine applications in \n",
    "restricted environments, employing a request -response model over UDP, featuring a client -server \n",
    "architecture us ing connection -less datagrams, and supporting HTTP -like methods such as GET, PUT, \n",
    "and DELETE.  \n",
    " \n",
    "Web  Socke t: protocol allows bidirectional communication over a single socket connection, letting \n",
    "clients and servers exchange messages seamlessly. It's based on TCP and keeps the connection open \n",
    "for continuous message exchange, catering to various clients like browsers, mobile apps, and IoT \n",
    "devices for real -time  \n",
    " \n",
    "MQTT  (Message Queuing Telemetry Transport):  is a lightweight protocol using a publish -\n",
    "subscribe model  where IoT devices connect to an MQTT broker to publish messages to topics, which \n",
    "are then forwarded to subscribed clients. Its efficiency and simplicity make MQTT well -suited for \n",
    "constrained environments.  \n",
    " \n",
    "XMPP  (Extensible Messaging and Presence Protocol ): enables  real-time communication and XML \n",
    "data streaming between network entities, supporting applications like messaging, presence, gaming, \n",
    "multiparty chat, and voice calls, while facilitating the transmission of small XML data chunks in real \n",
    "time across both client -to-server and server -to-client communication paths.  \n",
    " \n",
    "DDS  (Data Distribution Service) : facilitates machine -to-machine communication through a publish -\n",
    "subscribe model, where publishers create topics for subscribers to receive data, offering quali ty of \n",
    "service (QoS) control and configurable reliability for effective communication.  \n",
    " \n",
    "AMQP  (Advanced Message Queuing Protocol) : enables business messaging with support for \n",
    "routing and queuing through both point -to-point and publish -subscribe models. Broke rs in AMQP \n",
    "receive messages from publishers and deliver them to consumers over connections. Publishers \n",
    "transmit messages to exchanges, which then distribute copies to queues.  \n",
    " \n",
    " \n",
    " \n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "num_clusters=math.ceil(len(text)/3500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_clusters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# l=text.split(\". \")\n",
    "import re\n",
    "l=re.split(r'\\. |\\n',text)\n",
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "i=0\n",
    "while i<len(l):\n",
    "  if len(l[i])<=25:\n",
    "    l.remove(l[i])\n",
    "  else:\n",
    "    i=i+1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install textdistance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.cluster import KMeans\n",
    "import textdistance  # Make sure to install this library using: pip install textdistance\n",
    "\n",
    "# Sample string l\n",
    "# l = [\"apple\", \"banana\", \"orange\", \"grape\", \"cherry\", \"pineapple\", \"blueberry\", \"strawberry\"]\n",
    "\n",
    "# Function to calculate Levenshtein distance between two strings\n",
    "def levenshtein_distance(str1, str2):\n",
    "    return textdistance.levenshtein.normalized_distance(str1, str2)\n",
    "\n",
    "# Convert string l to a matrix of token counts\n",
    "# vectorizer = TfidfVectorizer(analyzer='char', use_idf=False)\n",
    "# X = vectorizer.fit_transform(l).toarray()\n",
    "\n",
    "# Calculate pairwise Levenshtein distances\n",
    "distances = np.zeros((len(l), len(l)))\n",
    "for i in range(len(l)):\n",
    "    for j in range(i+1, len(l)):\n",
    "        distances[i, j] = distances[j, i] = levenshtein_distance(l[i], l[j])\n",
    "\n",
    "# Perform k-means clustering\n",
    "# num_clusters = 6  # You can adjust the number of clusters as needed\n",
    "kmeans = KMeans(n_clusters=num_clusters, random_state=42)\n",
    "kmeans.fit(distances)\n",
    "\n",
    "# Add cluster labels to the original l\n",
    "cluster_labels = kmeans.labels_\n",
    "# result = pd.DataFrame({'String': l, 'Cluster': cluster_labels})\n",
    "# m=max(cluster_labels)+1\n",
    "# d={}\n",
    "# d2={}\n",
    "# for i in range(len(cluster_labels)):\n",
    "#   x=d.get(cluster_labels[i],\"\")+l[i]+\". \"\n",
    "#   if(len(x)>1200):\n",
    "#     if(cluster_labels[i] in d2):\n",
    "#       d[d2[cluster_labels[i]]]=d[d2[cluster_labels[i]]]+l[i]+\". \"\n",
    "#     else:\n",
    "#       d2[cluster_labels[i]]=m\n",
    "#       d[m]=l[i]+\". \"\n",
    "#       m+=1\n",
    "#   else:\n",
    "#     d[cluster_labels[i]]=x\n",
    "\n",
    "d={}\n",
    "for i in range(len(cluster_labels)):\n",
    "  d[cluster_labels[i]]=d.get(cluster_labels[i],\"\")+l[i]+\". \"\n",
    "\n",
    "# Display the result\n",
    "# print(result)\n",
    "l2=list(d.values())\n",
    "def split_long_strings(strings, max_length):\n",
    "    result = []\n",
    "    for string in strings:\n",
    "        if len(string) > max_length:\n",
    "            # Split the long string into parts of max_length\n",
    "            # parts = [string[i:i+max_length] for i in range(0, len(string), max_length)]\n",
    "            strs=string.split(\". \")\n",
    "            str1=strs[0]\n",
    "            for i in range(1,len(strs)):\n",
    "                if(len(str1+strs[i])<=max_length):\n",
    "                    str1+=strs[i]\n",
    "                else:\n",
    "                    result.append(str1)\n",
    "                    str1=strs[i]\n",
    "            result.append(str1)\n",
    "        else:\n",
    "            result.append(string)\n",
    "    return result\n",
    "\n",
    "# Example usage:\n",
    "cluster_list = split_long_strings(l2,3500)\n",
    "cluster_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain import PromptTemplate,  LLMChain\n",
    "def generate_summary(text_chunk):\n",
    "    # Defining the template to generate summary\n",
    "    template = \"\"\"\n",
    "    Context:\n",
    "    Give one line summary\n",
    "    Input text:\n",
    "    ```{text}```\n",
    "    SUMMARY:\n",
    "    \"\"\"\n",
    "    prompt = PromptTemplate(template=template, input_variables=[\"text\"])\n",
    "    llm_chain = LLMChain(prompt=prompt, llm=llm)\n",
    "\n",
    "    summary = llm_chain.run(text_chunk)\n",
    "    torch.cuda.empty_cache()\n",
    "    return summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pymongo\n",
    "!/opt/conda/bin/python3.10 -m pip install \"pymongo[srv]\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pymongo import MongoClient\n",
    "from bson import ObjectId\n",
    "connection_string=datasec[\"data\"]\n",
    "client = MongoClient(connection_string)\n",
    "db = client.Llama2\n",
    "collection = db.summary\n",
    "query_criteria = {\"_id\": ObjectId(id)}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "# chunk_summaries = []\n",
    "torch.cuda.empty_cache()\n",
    "import os\n",
    "\n",
    "combined_summary=\"\"\n",
    "# prev=\"\"\n",
    "# Set the max_split_size_mb environment variable\n",
    "os.environ['PYTORCH_CUDA_ALLOC_CONF'] = 'max_split_size_mb:50'\n",
    "for chunk in cluster_list:\n",
    "    summary = generate_summary(chunk)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(summary)\n",
    "    torch.cuda.empty_cache()\n",
    "    print(\"done\")\n",
    "    torch.cuda.empty_cache()\n",
    "    # Clear CUDA memory after each iteration\n",
    "    # sumlist=re.split(r'\\. |\\.\\n', summary)\n",
    "    # prev=sumlist[0][:280]+\". \"\n",
    "    # sumlist=summary.split(\"\\n\")\n",
    "    # prev=sumlist[0][:800]+\". \"\n",
    "    combined_summary+=summary.split(\"\\n\")[0]+\"\\n\"\n",
    "    \n",
    "    update_query = {\"$set\": {\"summary\": combined_summary}}\n",
    "    collection.update_one(query_criteria, update_query)\n",
    "    # chunk_summaries.append(summary)\n",
    "\n",
    "\n",
    "# combined_summary = \"\\n\".join(chunk_summaries)\n",
    "client.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(combined_summary)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [],
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
